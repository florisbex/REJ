\section{Discussion}
\label{sect:discussion}

\subsection{Related work}
\label{sect:goalmodeling:relatedwork}

\paragraph{Design Rationale} Argumentation in software design has for some time now been the subject of the work on so-called \emph{design rationale} (DR)~\cite{shum2006hypermedia}, an explicit documentation of the reasons behind decisions made when designing a system or software architecture. DR looks at issues, options and arguments for and against these options in the design of, for example, a software system. Similar to the literature on goal modeling, much of the traditional DR literature provides modeling languages and diagramming tool support for building design rationales. It is in this diagramming functionality that the link with argument diagrams from philosophy, law and AI~\cite{reed2004araucaria,gordon2007visualizing} has been made, where argument diagrams represent reasoning from premises to conclusions. More recent work on DR moves away from the idea that all decision have to be explicitly diagrammed and focuses more on empirically investigating how critical reflection can help when designing \cite{razavian2016two,SchriekEtal2016}, or which parts of the design process are best explicitly documented \cite{falessi2013value}. 

Software design and requirements engineering are very closely related \cite{nuseibeh2001weaving} and hence the insights from the DR literature are directly applicable to RE. The work on the RationalGRL framework essentially incorporates the core ideas from DR into goal-oriented requirements engineering by explicitly including arguments pro and con the various options into the goal model, and by proposing a methodology and critical questions that encourage reflection when thinking about the possible goals and functionality of a system. 

\paragraph{Requirements Engineering} There are a number of general approaches in the field of requirements engineering that explicitly take into account arguments. One early example comes from Haley et al.~\cite{haley2008security}, who use formal logical arguments to show that the system behavior satisfies certain security requirements, and more informal arguments to capture and validate the assumptions underlying the system behavior. This system behavior is defined by the tasks it executes and thus arguments are given for and against system tasks, similar to the way beliefs and counterarguments can be provided for tasks in RationalGRL. What Haley et al. leave implicit in their argumentation are the goals of the stakeholders on which the system tasks depend -- they include the goals in their framework and mention that there will often be conflicting goals between stakeholders, but do not explicitly model them. Furthermore, the argumentative part of their framework does not include formal semantics for resolving conflict between arguments or determining the acceptability of arguments. Yu et al.~\cite{yu2015automated} further extend the framework by Haley et al., including algorithms for Dung-style \cite{Dung1995} argumentation semantics and a database of specific ways in which to attack (or mitigate) risks, which can be likened to a set of critical questions for risks and security requirements (cf. Yu et al.~\cite{yu2015automated} Section 3.1). 

Another recent example of the use of arguments in goal-oriented requirements engineering is the work by Murukannaiah et al.~\cite{murukannaiah2015}, who propose Arg-ACH, an approach 
in which the beliefs underlying conflicting goals can be made explicit using argumentation. Murukannaiah et al. start with the basic technique of Analysis of Competing Hypotheses (ACH), where for conflicting goals the beliefs that are consistent and inconsistent with these goals are included in a matrix and counted. They then extend this technique into Arg-ACH: instead of just indicating whether a belief is consistent or inconsistent with a goal, each belief becomes an argument for or against the goal, which is then diagrammed using the Carneades tool \cite{gordon2007visualizing}. Belief scores are assigned to arguments, which can be aggregated to provide one's belief in a goal. The arguments for and against goals can be based on argument schemes, and critical questions can be used to find new arguments for or against the goals or the existing arguments. One example provided in the paper is the argument scheme from expert opinion, which allows one to draw conclusions based on expert statements and subsequently question, for example, the objectivity and veracity of the expert using critical questions. Murukannaiah et al. conducted an experiment in which they had two groups, one with ACH and one with Arg-ACH, perform an analysis of several conflicting goals regarding security at transport hubs. They found that, while the group that used Arg-ACH took longer, they also covered more possibilities in their belief search and the conclusions were more consistent among the group. 

One other example of argumentation in RE concerns the use of argumentation in requirements elicitation. Ionita et al.~\cite{ionita2014argumentation} propose an simple argumentation dialogue game in which risk assessors try to attack each other's arguments for certain risks attached to a system design. Dung's semantics \cite{Dung1995} are then used to determine the risks that are still valid, and those that have been successfully rejected. Elrakaiby et al.~\cite{ElrakaibyFSGN17} use argumentation to explain ambiguity. They define when a statement by a client who is being interviewed about the requirements of a system presents an inconsistency (either with the client's previous statements or with the requirement engineer's beliefs) or an insufficiency, that is, when an analyst needs more information from the client to accept a client's statement. The inconsistencies are then captured as mutually attacking arguments, and the insufficiencies as arguments against the original statements saying that, for example, the functionality expressed in the statement cannot be realised or is irrelevant. Elrakaiby et al. coded the data from 34 requirement elicitation interviews, identifying 39 inconsistencies (i.e. at least two arguments that mutually attack) and 29 insufficiencies (i.e., at least one argument attacking another).

It is clear from this literature that arguments play a core role in RE. Murukannaiah et al.~\cite{murukannaiah2015} show that critical reflection using argument schemes and critical questions -- in the same way that RationalGRL proposes -- improves the quality of the reasoning in the RE process. Elrakaiby et al.~\cite{ElrakaibyFSGN17} provide a case study similar to the current one, identifying, as we did, many counterarguments specifically with respect to realisability, relevance and clarity (cf. CQ2a, CQ3, CQ11 and CQ12 in Table~\ref{table:argument-schemes}). Like in RationalGRL, the use of Dung-style argumentation semantics to compute the acceptable claims in RE is further also advocated by the literature \cite{yu2015automated,ionita2014argumentation,ElrakaibyFSGN17}. 

The current work on RationalGRL puts the insights from the above-mentioned literature in a broader framework. For example, there is a specific focus on security requirements \cite{haley2008security,yu2015automated,ionita2014argumentation} or the reasoning is about single goals or tasks instead of about the wider context as represented in a goal model \cite{haley2008security,yu2015automated,murukannaiah2015,ElrakaibyFSGN17}. RationalGRL provides a generic and extensible framework for arguing about goals and tasks in RE. At the moment, there is only a ``generic argument'' in addition to arguments about goals and tasks. However, new argument schemes and critical questions about for example, security risks or expert opinions, can be easily added: the metamodel (Figure~\ref{fig:metamodel}) accommodates this and the formal specification in Section~\ref{sect:formalframework} is set up in such a way that  extending the definition of argument and adding new algorithms for specific critical questions is easy. 

\paragraph{Goal Modeling} Argumentation has been included -- both explicitly and implicitly -- in existing goal modeling languages. For example, the belief element in the original GRL specification \cite{Amyot:2010:EGM:1841349.1841356} is meant to capture the rationales (reasons, arguments) behind the inclusion of goals and tasks in the model. Furthermore, relations between elements in a goal model also provide justifications: high-level goals are reasons for lower-level goals, tasks and resources. Hence, refinement and decomposition techniques used in requirements engineering \cite{van2001goal} can be seen as explicit argumentation steps in the goal modeling process. Take, for example, CQ2 of PRAS (Section~\ref{sect:background:pras}), which asks whether there are alternative ways of realizing the same goal. Providing an alternative sub-goal or -task in a goal model then an explicit argumentative move in the discussion. So in this sense, a goal model provides a justification for itself, particularly if we include belief elements for extra design rationalization. This idea is also prevalent in our RationalGRL framework: many arguments are in fact GRL elements, and many critical questions can be answered by introducing new GRL elements. However, as was already argued in Section~\ref{sect:introduction} (and also by \cite{Jureta:RE2008}), argumentation produces different, richer and complementary information to just the goal model. The goal model is the product of a process of argumentation, and does not include, for example, goals and tasks that were at some point considered but discarded. Furthermore, for goal models it is only possible to determine the satisfiability of goals given the possible tasks and resources; what cannot be determined is the acceptability of goals, that is, whether they are acceptable given potentially contradictory opinions of stakeholders.  

There are several contributions to the literature that relate argumentation-based techniques with goal modeling. The contribution most closely related to ours is the work by Jureta \emph{et al.}~\cite{Jureta:RE2008}. Jureta \emph{et al.} propose ``Goal Argumentation Method (GAM)'' to guide argumentation and justification of modeling choices during the construction of goal models. One of the elements of GAM is the translation of formal argument models to goal models (similar to ours). In this sense, our RationalGRL framework can be seen as an instantiation and implementation of  part of the GAM. The main difference between our approach and GAM is that we integrate arguments and goal models using argument schemes, and that we develop these argument schemes by analyzing transcripts. GAM instead uses structured argumentation. 

In previous work on the RationalGRL framework~\cite{vanzee-etal:renext2015,vanZee-etal:er2016}, we have explored one way of combining PRAS and GRL. In this work, which takes a similar approach to \cite{Jureta:RE2008}, argument diagrams are translated to GRL models (an automatic translation tool is discussed in~\cite{vanZee-etal:comma2016}). The argument diagrams are complex practical reasoning arguments, where the premises are the goals and the conclusion is some action we need to perform to realize those goals. Thus, we have essentially two complex diagrams, a practical reasoning argument diagram and a GRL goal diagram, and a mapping between them. 

Connecting argumentation and goal modelling by providing a mapping between two types of diagram was, in our opinion, ultimately an unsatisfying solution given the problems and requirements described in Section \ref{sect:introduction}. One problem is that the argument diagram is at least as complex as the GRL diagram, so any stakeholder trying to understand the discussion has to parse two complex diagrams containing goals, alternative solutions, tasks, and so forth. Furthermore, in our previous work we did not provide the specific critical questions for goal models (see Section \ref{sect:gmas}). This meant that any counterargument had to be constructed from scratch, as no guidance was given as to possible ways to criticize a GRL model. So the previous iterations of the RationalGRL framework violated requirement 1: argument diagrams do not closely mirror the actual discussions of stakeholders in which ideas are proposed and challenged. Furthermore, not having specific argument schemes and critical questions for goal modelling makes it hard to develop a guiding methodology for the use of argumentation in goal modelling (requirement 4). 

One idea is to capture requirements engineering and software design processes as explicit dialogues between parties~\cite{finkelstein1989multiparty}. 

\todo{F}{F}{Review the following:

\begin{itemize}
\item Mirbel and Villata on Dung \& goal models
\item Software design discussions Black et al., Prakken and Wierenga, \cite{finkelstein1989multiparty}
  \end{itemize}
}
\subsection{Open issues}
\label{sect:goalmodeling:openissues}

We see a large number of open issues that we hope will be explored in future research. We discuss five promising directions here.

\subsubsection*{Architecture principles} 
One aspect of enterprise architecture that we did not touch upon in this article are \emph{(enterprise) architecture principles}. Architecture principles are general rules and guidelines, intended to be enduring and seldom amended, that inform and support the way in which an organization sets about fulfilling its mission~\cite{Lankhorst2005,OptLand2007a,OG2009}. They reflect a level of consensus among the various elements of the enterprise, and form the basis for making future IT decisions. Two characteristics of architecture principles are:
\begin{itemize}
\item There are usually a small number of principles (around 10) for an entire organization. These principles are developed by enterprise architecture, through discussions with stakeholders or the executive board. Such a small list is intended to be understood \emph{throughout the entire organization}. All employees should keep these principles in the back of their hard when making a decision.
\item Principles are meant to guide decision making, and if someone decides to deviate from them, he or she should have a good reason for this and explain why this is the case. As such, they play a normative role in the organization.
\end{itemize}

Looking at these two characteristics, we see that argumentation, or justification, plays an important role in both forming the principles and adhering to them:
\begin{itemize}
\item Architecture principles are \emph{formed} based on underlying arguments, which can be the goals and values of the organization, preferences of stakeholders, environmental constraints, etc.
\item If architecture principles are \emph{violated}, this violation has to be explained by underlying arguments, which can be project-specific details or lead to a change in the principle.
\end{itemize}

In a previous paper, we~\cite{marosin-etal:caise2016} propose an extension to GRL based on enterprise architecture principles. We present a set of requirements for improving the clarity of definitions and develop a framework to formalize architecture principles in GRL. We introduce an extension of the language with the required constructs and establish modeling rules and constraints. This allows one to automatically reason about the soundness, completeness and consistency of a set of architecture principles. Moreover, principles can be traced back to high-level goals.

It would be very interesting future work to combine the architecture principles extension with the argumentation extension. This would lead to a framework in which principles cannot only be traced back to goals, but also to underlying arguments by the stakeholders.

\subsubsection*{Extensions for argumentation}

The amount of argumentation theory we used in this article has been rather small. Our intention was to create a bridge between the formal theories in argumentation and the rather practical tools in requirements engineering. Now that the initial framework has been developed, is it worth exploring what tools and variations formal argumentation has to offer in more detail.

For instance, until now we have assumed that every argument put forward by a critical questions always defeats the argument it questions, but this is a rather strong assumption.  In some cases, it is more difficult to determine whether or not an argument is defeated. Take, for example, the argumentation framework in Figure~\ref{fig:goalmodeling:futureargs} with just A1 and A2. These two arguments attack each other, they are alternatives and without any explicit preference, and it is impossible to choose between the two. It is, however, possible to include explicit preferences between arguments when determining argument acceptability \cite{amgoud2002reasoning}. If we say that we prefer the action  \texttt{Create new cars} (A2) over the action  \texttt{Keep same cars} (A1), we remove the attack from A1 to A2. This makes A2 the only undefeated argument, whereas A1 is now defeated. It is also possible to give explicit arguments for preferences~\cite{modgil2009}. These arguments are then essentially attacks on attacks. For example, say we prefer A3 over A1 because `it is important to have realistic traffic flows' (A4). This can be rendered as a separate argument that attacks the attack from A1 to A3, removing this attack and making $\{$A3, A4$\}$ the undefeated set of arguments.

Allowing undefeated attacks also make the question of which semantics to choose more interested. In our current (a-cyclic) setting, all semantics coincide, and we always have the same set of accepted arguments. However, once we allow for cycles, we may choose accepted arguments based on semantics which, for instance, try to accept/reject as many arguments as possible (preferred semantics), or just do not make any choice once there are multiple choices (grounded). Another interesting element of having cycles is that one can have multiple extensions. This corresponds to various \emph{positions} are possible, representing various sets of possibly accepted arguments. Such sets can then be shown to the user, who can then argue about which one they deem most appropriate.


\begin{figure}[ht]
\centering
\begin{tikzpicture}
        \node[minimum size=1cm] (a1) [argNodeIN] at (0,0) {$A_1$};
        \node[minimum size=1cm] (a2) [argNodeIN] at (3,0) {$A_2$};
        
         \path
    (a2) edge [attackLink] (a1)
    (a1) edge [attackLink] (a2);
\end{tikzpicture}
\caption{Preferences between arguments}
\label{fig:goalmodeling:futureargs}
\end{figure}  

Finally, in this article we have only explored one single argument scheme, but there are many other around. In his famous book ``Argumentation schemes'', Walton describes a total of 96 schemes. Murukannaiah \emph{et al.}~\cite{murukannaiah2015} already explain how some of these schemes may be use for resolving goal conflicts, and it is worth studying what this would look like in our framework as well.

\subsubsection*{Empirical study}

Although we develop our argument schemes and critical questions with some empirical data, we did not yet validate the outcome. This is an important part, because it will allow us to understand whether adding arguments to goal modeling is actually useful. We have developed an experimental setup for our experiment, which we intend to do during courses at various universities. However, we cannot carry out this experiment until the tool is finished.

\subsubsection*{Formal framework}

The formal framework we present in this article is very simple, and does not provide a lot of detail. We believe it would be interesting to develop a more robust characterization of a GRL model using logical formulas. Right now, we have no way to verify whether the goal models we obtain through out algorithms are actually valid GRL models. This is because we allow any set of atoms to be a GRL model, which is clearly very permissive and incorrect. Once we develop a number of such constraints, we can ensure (and even proof) our algorithms do not generate invalid GRL models. 

For instance, suppose we assert that an \emph{intentional element} is a goal, softgoal, task, or resource: \begin{align*}
(softgoal(i)\vee goal(i)\vee task(i)\\
\vee resource(i)\rightarrow IE(i).
\end{align*}
We can then formalize an intuition such as: ``Only intentional elements can be used in contribution relations'' as follows
\begin{align*}
contrib(k,i,j,ctype)\rightarrow \\
(IE(i)\wedge IE(j)\wedge IE(j).
\end{align*}

Interestingly, such constraints are very comparable to \emph{logic programming} rules. We therefore see it as interesting future research to explore this further, specifically in the following two ways:
\begin{itemize}
\item Develop a set of constraints on sets of atoms of our language, which correctly describe a GRL model. Show formally that using our algorithms, each extension of the resulting argumentation framework corresponds to a valid GRL model, i.e., a GRL model that does not violate any of the constraints.
\item Implement the constraints as a logic program, and use a logic programming language to compute the resulting GRL model.
\end{itemize}


\subsection{Conclusion}

\todo{Marc}{all}{Finish this}

The introduction of this article contains five requirements we identified for our framework. We use the conclusion to discuss how RationalGRL meets our initial requirements.

\paragraph{1. The argumentation techniques should be close to actual discussions stakeholders or designers have.}

We analyze a set of transcripts containing discussions about the architecture of an information system. 

\paragraph{2. The framework must have the means to formally model parts of the discussion process.}
In order to generate goal models based on formalized discussions (requirement 2), we, first, formalize the list of arguments from requirement 1 in an argumentation framework. We formalize the critical questions as algorithms modifying the argumentation framework. We use argumentation techniques from AI in order to determine which arguments are accepted and which are rejected. We propose an algorithm to generate a GRL model based on the accepted arguments. This helps providing traceability links from GRL elements to the underlying arguments (requirement 3).

We implement our framework in an online tool called RationalGRL (requirement 4). The tool is implemented using Javascript. It contains  two parts, goal modeling and argumentation. The goal modeling part is a simplified version of GRL, leaving out features such as evaluation algorithms and key performance indicators. The argumentation part is new, and we develop a modeling language for the arguments and critical questions. The created GRL models in RationalGRL can be exported to jUCMNav~\cite{} %TODO SG: add citation 
 the Eclipse-based tool for GRL modeling, for further evaluation and analysis. 

Our final contribution is a methodology on how to develop goal models that are linked to underlying discussions. The methodology consists of two parts, namely argumentation and goal modeling. In the argumentation part, one puts forward arguments and counter-arguments by applying critical questions. When switching to the goal modeling part, the accepted arguments are used to create a goal model. In the goal modeling part, one simply modifies goal models, which may have an effect on the underlying arguments. This might mean that the underlying arguments are no longer consistent with the goal models.