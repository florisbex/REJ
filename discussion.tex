\section{Discussion}
\label{sect:discussion}

\subsection{Related work}
\label{sect:goalmodeling:relatedwork}

The need for justifications of modeling choices plays an important role in different requirements engineering methods using goal models. The belief element in the original GRL specification \cite{Amyot:2010:EGM:1841349.1841356} is meant to capture the rationales behind the inclusion of goals and tasks in the model. Furthermore, relations between elements in a goal model also provide justifications: high-level goals are reasons for lower-level goals, tasks and resources. Hence, refinement and decomposition techniques used in requirements engineering \cite{van2001goal} can be seen as explicit argumentation steps in the goal modeling process. Take, for example, CQ2 of PRAS (Section~\ref{sect:background:pras}), which asks whether there are alternative ways of realizing the same goal. Providing an alternative sub-goal or -task in a goal model then an explicit argumentative move in the discussion. 

So in a sense, a goal model provides a justification for itself, particularly if we include belief elements for extra design rationalization. This idea is also prevalent in our RationalGRL framework: many arguments are in fact GRL elements, and many critical questions can be answered by introducing (\textsf{INTRO}) new GRL elements. However, as was already argued in Section~\ref{sect:introduction} (and also by \cite{Jureta:RE2008}), argumentation produces different, richer and complementary information to just the goal model. The goal model is the product of a process of argumentation, and does not include, for example, goals and tasks that were at some point considered discarded. Furthermore, for goal models it is only possible to determine the satisfiability of goals given the possible tasks and resources; what cannot be determined is the acceptability of goals, that is, whether they are acceptable given potentially contradictory opinions of stakeholders.  

Closely related to goal modeling and design justification are frameworks that aim to provide a design rationale (DR)~\cite{shum2006hypermedia}, an explicit documentation of the reasons behind decisions made when designing a system or artefact. DR looks at issues, options and arguments for and against these options in the design of, for example, a software system. The similarity with goal modeling is that much of the traditional DR literature provides modeling languages and tool support for building DR diagrams. The differences lie in the concepts used: in DR, the focus is more on the rationales, why certain design options were chosen, than on the functional and non-functional requirements of the system, which are expressed in goal models. The RationalGRL framework in a sense incorporates the ideas from DR into goal models by including arguments and issues (critical questions) into the RatinalGRL model.

Arguments and requirements engineering approaches have been combined by, among others, Haley \emph{et al.}~\cite{haley2005arguing}, who use structured arguments to capture and validate the rationales for security requirements. However, they do not use goal models, and thus, there is no explicit trace from arguments to goals and tasks. Furthermore, like~\cite{Jureta:RE2008}, the argumentative part of their work does not include formal semantics for determining the acceptability of arguments, and the proposed frameworks are not actually implemented. Murukannaiah \emph{et al.}~\cite{murukannaiah2015resolving} propose Arg-ACH, an approach to capture inconsistencies between stakeholders' beliefs and goals, and resolve goal conflicts using argumentation techniques.

There are several contributions to the literature that relate argumentation-based techniques with goal modeling. The contribution most closely related to ours is the work by Jureta \emph{et al.}~\cite{Jureta:RE2008}. Jureta \emph{et al.} propose ``Goal Argumentation Method (GAM)'' to guide argumentation and justification of modeling choices during the construction of goal models. One of the elements of GAM is the translation of formal argument models to goal models (similar to ours). In this sense, our RationalGRL framework can be seen as an instantiation and implementation of  part of the GAM. The main difference between our approach and GAM is that we integrate arguments and goal models using argument schemes, and that we develop these argument schemes by analyzing transcripts. GAM instead uses structured argumentation. 

In previous work on the RationalGRL framework~\cite{vanzee-etal:renext2015,vanZee-etal:er2016}, we have explored one way of combining PRAS and GRL. In this work, which takes a similar approach to \cite{Jureta:RE2008}, argument diagrams are translated to GRL models (an automatic translation tool is discussed in~\cite{vanZee-etal:comma2016}). The argument diagrams are complex practical reasoning arguments, where the premises are the goals and the conclusion is some action we need to perform to realize those goals. Thus, we have essentially two complex diagrams, a practical reasoning argument diagram and a GRL goal diagram, and a mapping between them. 

Connecting argumentation and goal modelling by providing a mapping between two types of diagram was, in our opinion, ultimately an unsatisfying solution given the problems and requirements described in Section \ref{sect:introduction}. One problem is that the argument diagram is at least as complex as the GRL diagram, so any stakeholder trying to understand the discussion has to parse two complex diagrams containing goals, alternative solutions, tasks, and so forth. Furthermore, in our previous work we did not provide the specific critical questions for goal models (see Section \ref{sect:gmas}). This meant that any counterargument had to be constructed from scratch, as no guidance was given as to possible ways to criticize a GRL model. So the previous iterations of the RationalGRL framework violated requirement 1: argument diagrams do not closely mirror the actual discussions of stakeholders in which ideas are proposed and challenged. Furthermore, not having specific argument schemes and critical questions for goal modelling makes it hard to develop a guiding methodology for the use of argumentation in goal modelling (requirement 4). 

\todo{Marc}{all}{Review the following notes:

\begin{itemize}
\item RE17 paper: merging argumentation theory with law
  "Using Argumentation to Explain Ambiguity in Requirements Elicitation Interviews" Yehia Elrakaiby, Alessio Ferrari, Paola Spoletini, Stefania Gnesi and Bashar Nuseibeh
\item more details on Murukannaiah [RE15]
\item related work on argumentation theory - check Serena Villata on goal models
\item Prakken and Wierenga
\item First paper we wrote they also said something about Bashar Nuseibeh
\item Also mention argumentation in AI - arg diagramming - and how that relates to our work
\item Software design discussions Black et al.
\item Maybe this one:
  "Complete Traceability for Requirements in Satisfaction Arguments"
  Anitha Murugesan, Michael Whalen, Elaheh Ghassabani and Mats Heimdahl
  \end{itemize}
}
\subsection{Open issues}
\label{sect:goalmodeling:openissues}

We see a large number of open issues that we hope will be explored in future research. We discuss five promising directions here.

\subsubsection*{Architecture principles} 
One aspect of enterprise architecture that we did not touch upon in this article are \emph{(enterprise) architecture principles}. Architecture principles are general rules and guidelines, intended to be enduring and seldom amended, that inform and support the way in which an organization sets about fulfilling its mission~\cite{Lankhorst2005,OptLand2007a,OG2009}. They reflect a level of consensus among the various elements of the enterprise, and form the basis for making future IT decisions. Two characteristics of architecture principles are:
\begin{itemize}
\item There are usually a small number of principles (around 10) for an entire organization. These principles are developed by enterprise architecture, through discussions with stakeholders or the executive board. Such a small list is intended to be understood \emph{throughout the entire organization}. All employees should keep these principles in the back of their hard when making a decision.
\item Principles are meant to guide decision making, and if someone decides to deviate from them, he or she should have a good reason for this and explain why this is the case. As such, they play a normative role in the organization.
\end{itemize}

Looking at these two characteristics, we see that argumentation, or justification, plays an important role in both forming the principles and adhering to them:
\begin{itemize}
\item Architecture principles are \emph{formed} based on underlying arguments, which can be the goals and values of the organization, preferences of stakeholders, environmental constraints, etc.
\item If architecture principles are \emph{violated}, this violation has to be explained by underlying arguments, which can be project-specific details or lead to a change in the principle.
\end{itemize}

In a previous paper, we~\cite{marosin-etal:caise2016} propose an extension to GRL based on enterprise architecture principles. We present a set of requirements for improving the clarity of definitions and develop a framework to formalize architecture principles in GRL. We introduce an extension of the language with the required constructs and establish modeling rules and constraints. This allows one to automatically reason about the soundness, completeness and consistency of a set of architecture principles. Moreover, principles can be traced back to high-level goals.

It would be very interesting future work to combine the architecture principles extension with the argumentation extension. This would lead to a framework in which principles cannot only be traced back to goals, but also to underlying arguments by the stakeholders.

\subsubsection*{Extensions for argumentation}

The amount of argumentation theory we used in this article has been rather small. Our intention was to create a bridge between the formal theories in argumentation and the rather practical tools in requirements engineering. Now that the initial framework has been developed, is it worth exploring what tools and variations formal argumentation has to offer in more detail.

For instance, until now we have assumed that every argument put forward by a critical questions always defeats the argument it questions, but this is a rather strong assumption.  In some cases, it is more difficult to determine whether or not an argument is defeated. Take, for example, the argumentation framework in Figure~\ref{fig:goalmodeling:futureargs} with just A1 and A2. These two arguments attack each other, they are alternatives and without any explicit preference, and it is impossible to choose between the two. It is, however, possible to include explicit preferences between arguments when determining argument acceptability \cite{amgoud2002reasoning}. If we say that we prefer the action  \texttt{Create new cars} (A2) over the action  \texttt{Keep same cars} (A1), we remove the attack from A1 to A2. This makes A2 the only undefeated argument, whereas A1 is now defeated. It is also possible to give explicit arguments for preferences~\cite{modgil2009}. These arguments are then essentially attacks on attacks. For example, say we prefer A3 over A1 because `it is important to have realistic traffic flows' (A4). This can be rendered as a separate argument that attacks the attack from A1 to A3, removing this attack and making $\{$A3, A4$\}$ the undefeated set of arguments.

Allowing undefeated attacks also make the question of which semantics to choose more interested. In our current (a-cyclic) setting, all semantics coincide, and we always have the same set of accepted arguments. However, once we allow for cycles, we may choose accepted arguments based on semantics which, for instance, try to accept/reject as many arguments as possible (preferred semantics), or just do not make any choice once there are multiple choices (grounded). Another interesting element of having cycles is that one can have multiple extensions. This corresponds to various \emph{positions} are possible, representing various sets of possibly accepted arguments. Such sets can then be shown to the user, who can then argue about which one they deem most appropriate.


\begin{figure}[ht]
\centering
\begin{tikzpicture}
        \node[minimum size=1cm] (a1) [argNodeIN] at (0,0) {$A_1$};
        \node[minimum size=1cm] (a2) [argNodeIN] at (3,0) {$A_2$};
        
         \path
    (a2) edge [attackLink] (a1)
    (a1) edge [attackLink] (a2);
\end{tikzpicture}
\caption{Preferences between arguments}
\label{fig:goalmodeling:futureargs}
\end{figure}  

Finally, in this article we have only explored one single argument scheme, but there are many other around. In his famous book ``Argumentation schemes'', Walton describes a total of 96 schemes. Murukannaiah \emph{et al.}~\cite{murukannaiah2015} already explain how some of these schemes may be use for resolving goal conflicts, and it is worth studying what this would look like in our framework as well.

\subsubsection*{Empirical study}

Although we develop our argument schemes and critical questions with some empirical data, we did not yet validate the outcome. This is an important part, because it will allow us to understand whether adding arguments to goal modeling is actually useful. We have developed an experimental setup for our experiment, which we intend to do during courses at various universities. However, we cannot carry out this experiment until the tool is finished.

\subsubsection*{Formal framework}

The formal framework we present in this article is very simple, and does not provide a lot of detail. We believe it would be interesting to develop a more robust characterization of a GRL model using logical formulas. Right now, we have no way to verify whether the goal models we obtain through out algorithms are actually valid GRL models. This is because we allow any set of atoms to be a GRL model, which is clearly very permissive and incorrect. Once we develop a number of such constraints, we can ensure (and even proof) our algorithms do not generate invalid GRL models. 

For instance, suppose we assert that an \emph{intentional element} is a goal, softgoal, task, or resource: \begin{align*}
(softgoal(i)\vee goal(i)\vee task(i)\\
\vee resource(i)\rightarrow IE(i).
\end{align*}
We can then formalize an intuition such as: ``Only intentional elements can be used in contribution relations'' as follows
\begin{align*}
contrib(k,i,j,ctype)\rightarrow \\
(IE(i)\wedge IE(j)\wedge IE(j).
\end{align*}

Interestingly, such constraints are very comparable to \emph{logic programming} rules. We therefore see it as interesting future research to explore this further, specifically in the following two ways:
\begin{itemize}
\item Develop a set of constraints on sets of atoms of our language, which correctly describe a GRL model. Show formally that using our algorithms, each extension of the resulting argumentation framework corresponds to a valid GRL model, i.e., a GRL model that does not violate any of the constraints.
\item Implement the constraints as a logic program, and use a logic programming language to compute the resulting GRL model.
\end{itemize}


\subsection{Conclusion}

\todo{Marc}{all}{Finish this}

The introduction of this article contains five requirements we identified for our framework. We use the conclusion to discuss how RationalGRL meets our initial requirements.

\paragraph{1. The argumentation techniques should be close to actual discussions stakeholders or designers have.}

We analyze a set of transcripts containing discussions about the architecture of an information system. 

\paragraph{2. The framework must have the means to formally model parts of the discussion process.}
In order to generate goal models based on formalized discussions (requirement 2), we, first, formalize the list of arguments from requirement 1 in an argumentation framework. We formalize the critical questions as algorithms modifying the argumentation framework. We use argumentation techniques from AI in order to determine which arguments are accepted and which are rejected. We propose an algorithm to generate a GRL model based on the accepted arguments. This helps providing traceability links from GRL elements to the underlying arguments (requirement 3).

We implement our framework in an online tool called RationalGRL (requirement 4). The tool is implemented using Javascript. It contains  two parts, goal modeling and argumentation. The goal modeling part is a simplified version of GRL, leaving out features such as evaluation algorithms and key performance indicators. The argumentation part is new, and we develop a modeling language for the arguments and critical questions. The created GRL models in RationalGRL can be exported to jUCMNav~\cite{} %TODO SG: add citation 
 the Eclipse-based tool for GRL modeling, for further evaluation and analysis. 

Our final contribution is a methodology on how to develop goal models that are linked to underlying discussions. The methodology consists of two parts, namely argumentation and goal modeling. In the argumentation part, one puts forward arguments and counter-arguments by applying critical questions. When switching to the goal modeling part, the accepted arguments are used to create a goal model. In the goal modeling part, one simply modifies goal models, which may have an effect on the underlying arguments. This might mean that the underlying arguments are no longer consistent with the goal models.